One of the main aspects of machine learning methods is the implementation of learning rates. Selecting the correct learning rate method is a big problem since different learning rate methods change learning rates during the training differently; hence, the same model learns differently. This paper uses different learning rate optimizer methods implemented from cognitive bias methods for GLVQ. The cognitive (science) learning rate optimizers have been researched with one of the sub-models of the LVQ model; however, not with the GLVQ model. To uncover the performance of the learning rate methods with GLVQ, in this paper, we investigate the learning rate changes during the training using some datasets that offered open-source and extra datasets we created.

LVQ is a prototype-based machine learning method that Kohonen (1995) introduced in his work “Self-Organizing Maps”~\cite{kohonen1}. LVQ is a computing-friendly machine learning model. According to Kohonen (1990), in another work, LVQ has similar accuracy rates to Neural Networks with smaller computing power~\cite{kohonen2}. Instead of weight vectors such as Neural Network algorithms, LVQ, and branches of LVQ, use part of the data to train the model for classification~\cite{kohonen1}. LVQ provides a more human-like learning system than Neural Network provides. We use the GLVQ model in this paper.

There are several learning rate methods introduced throughout the beginning of LVQ. One of them we use is optimized GLVQ (OGLVQ). Changing any LVQ model to an optimized version is mentioned in the “Self-Organizing Maps” book by Kohonen (1995)~\cite{kohonen1}. We show how to optimize GLVQ to OGLVQ in our paper while mentioning the GLVQ method.

Cognitive learning rate optimizers have been mentioned by Takahashi et al. (2010) in the paper “Cognitive Symmetry: Illogical but Rational Biases”; these cognitive learning rate optimizers are CP (Conditional Probability), DP (Contingency Model), DFH (Dual Factor Heuristic), RS (Rigidly Symmetric), MS (Middle Symmetry), LS (Loose Symmetry), and LSR (Loose Symmetry with Rarity)~\cite{cogn,shinohara}. However, the research lacks a learning rate analysis. The methods CP, DFH, MS, LS, and LSR show high performance (with $>.9$ determination coefficients) on \textit{human} data from the paper “Contributions of specific cell information to judgments of interevent contingency” by Wasserman et al. (1990)~\cite{wasser} done by Takahashi et al. (2010)~\cite{cogn}. These cognitive learning rate methods are valuable for further research, which we do in this paper.

In addition to CP, LS from cognitive learning rate methods, eLS (enhanced loose symmetric) introduced and compared performance against famous machine learning classification methods such as neural networks (NN), support vector machine (SVM), random forest (RF), and logistic regression (LR) in the paper “A machine learning model with human cognitive biases capable of learning from small and biased datasets” (Taniguchi et al., 2018)~\cite{els}. The cognitive learning rate methods are implemented under the Naïve Bias model. The results include accuracy and F1 scores analysis, showing that cognitive models compete well with other classification models.

Some of these learning methods we discussed, CP, RS, and LS, have been analyzed deeply under the Self-incremental LVQ (SILVQ) model in the paper “Self-incremental learning vector quantization with human cognitive biases” (Manome et al., 2021)~\cite{lrimp}. The paper also includes analysis under the learning rate change with one dataset, \textit{Glass} dataset~\cite{glass,lrimp}. The paper compares the performances of OLVQ and LVQ with different initial learning rates and SILVQ with three cognitive learning methods. We extend this research with new datasets and a different base model, GLVQ.

Learning of the LVQ model is present if the learning rate is decreasing near 0 during the training period. Accuracy can still increase in some cases, even though learning rates do not change, corresponding to no learning. So, just examining accuracy scores does not give us much answer if the model is learning or not. However, examining the learning rate change during the training would give us some answers. The papers by Takahashi et al. (2010)~\cite{cogn} and Taniguchi et al. (2018)~\cite{els} lack a deep analysis of the learning rates. We included the best-performing cognitive learning rate methods from the paper “Cognitive Symmetry: Illogical but Rational Biases”~\cite{cogn}, CP, DFH, MS, LS, and LSR, in our analysis, including OGLVQ as a comparison model. Our task is to discover the learning rate analysis on the GLVQ model, using the mentioned learning rate methods and supporting the results with accuracy and F-1 scores. For this task, we use various datasets, some of these datasets used in the paper “Self-incremental learning vector quantization with human cognitive biases” by Manome et al. (2021)~\cite{lrimp}: \textit{Ionosphere} dataset~\cite{ion}, \textit{Iris} dataset~\cite{iris}, and \textit{Sonar} dataset~\cite{sonar}, additionally an open-source \textit{Breast Cancer Wisconsin} dataset~\cite{cancer}, and custom IFE Blood Samples datasets: \textit{NSP} and \textit{SP} datasets created by Saruhan (2023) in the report “Informational Image Data Pre-processing: IFE Blood Samples” ~\cite{mypap}. In this paper, we name GLVQ models, which use learning rate optimizers implemented from cognitive learning rate methods as CGLVQ (cognitive GLVQ) to simplify the refer of the group.
