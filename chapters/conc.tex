With our experiments, we investigated the CGLVQ models concerning their learning rate changes and revealed the power of the models. The learning capability of humankind is higher than that of other species in the world, at least as we know it now. Also, learning might not be logical like machines. Tracing the human biases on machine learning models helps us to copy human-like learning, which allows us to create more human-like machines.

Unfortunately, our custom datasets, \textit{NSP} and \textit{SP}, were untrainable with our setup of the models; hence, we could not get much information from these datasets. On the other hand, with other datasets we used, which are open-source datasets, we found CGLVQ is adapting the sample spaces of these datasets and learning through the training. We had great results with our Experiment 1, which has datasets with balanced class samples for the training. In Experiment 1, CGLVQ models, except the CP model, are as competitive as the OGLVQ model. Even with the \textit{Ionosphere} dataset, CGLVQ models outperform OGLVQ model. This experiment found that MS and LSR models perform better with the \textit{Ionosphere} dataset. In contrast, LSR has the only good performance with the \textit{Iris} dataset compared to other CGLVQ models.


Experiment 2 showed us another power of the CGLVQ models. We used imbalanced datasets on the models by reducing the sample size for one of the classes in the datasets. With an imbalanced dataset experiment, we found that there are 3 CGLVQ models that shine: MS, LS, and LSR. These models outperformed OGLVQ with imbalanced and small datasets, \textit{Ionosphere} and \textit{Sonar} datasets, and showed better learning rate graphs in their results. The LS model showed the biggest performances among other CGLVQ models concerning the F1 score.

One thing to note is that CGLVQ uses class-based learning rates rather than prototype-based ones. This behavior might seem like a bad idea in theory since the sample space might not be divided linearly, and updating the learning rates of the prototypes in the same class might cause problems. However, our experiments show great results, even better than OGLVQ. Still, because of the class-based learning rate adaptation, these CGLVQ models might perform worse than other LVQ models in a noisier sample space.

In summary, we found in this paper that the cognitive science learning rate optimizer approaches have great results, especially MS, LS, and LSR. These results align with the findings of Takahashi et al. (2010)~\cite{cogn}. However, the findings of Takahashi et al. (2010)~\cite{cogn} show good performance with CP and DFH learning rate methods according to Table~\ref{cogntable}. Still, our results found that these optimizers do not perform much compared to other learning rate methods. So, we can conclude that we can continue more research to improve MS, LS, and LSR. There are many more cognitive science learning rate methods created from cognitive biases to optimize the learning rates that we can use on GLVQ models we did not include. One can expand the research, including other models we did not include and new models based on cognitive biases.

Since we noted that we used and experimented on datasets with simple (not noisy) sample space, it would be great to have further experiments with (reasonably) nosier datasets. In that way, we can see how the selected CGLVQ models generally perform with noisier datasets. Solving less noisy datasets is easy for most models. However, we see the models’ usefulness when the dataset is harder to solve since it is closer to real-world problems.

Finally, we can say that CGLVQ models, especially MS, LS, and LSR, have better results than OGLVQ model. Since CGLVQ learning rate optimizers come from cognitive science, the models provide more human-like learning for machine learning, which helps us to create more human-like models while also allowing us to understand the model’s reasoning.
